{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:33:12.363673Z",
     "end_time": "2023-04-16T22:33:12.370522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Activate logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:33:12.368075Z",
     "end_time": "2023-04-16T22:33:16.212954Z"
    }
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:33:21.409182Z",
     "end_time": "2023-04-16T22:33:27.913886Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/haystack-test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore(\n",
    "    similarity=\"cosine\",\n",
    "    index=\"document\",\n",
    "    embedding_field=\"embedding\",\n",
    "    embedding_dim=1536\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T18:17:28.383112Z",
     "start_time": "2023-04-06T18:17:27.131964Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - datasets.builder -  Using custom data configuration bilgeyucel--seven-wonders-6536ac0e9c84233f\n",
      "WARNING - datasets.builder -  Found cached dataset parquet (/Users/rjtannous/.cache/huggingface/datasets/bilgeyucel___parquet/bilgeyucel--seven-wonders-6536ac0e9c84233f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load seven wonders dataset from datasets and write documents into document store\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "document_store.write_documents(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:33:35.970688Z",
     "end_time": "2023-04-16T22:33:36.166173Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU - Number of GPUs: 0\n",
      "INFO - haystack.nodes.retriever.dense -  Init retriever using embeddings of model text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "# define retriever model , update dense vector embeddings on document store index and define generator model\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"text-embedding-ada-002\",\n",
    "    api_key=openai_api_key,\n",
    "    max_seq_len=1024,\n",
    "    top_k=4,\n",
    ")\n",
    "document_store.update_embeddings(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:33:42.896612Z",
     "end_time": "2023-04-16T22:33:42.899563Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the QA tool's prompt template\n",
    "\n",
    "from haystack.nodes import PromptNode, PromptTemplate\n",
    "\n",
    "QA_promptnode = PromptTemplate(\n",
    "            name=\"zero-shot-QA\", \n",
    "            prompt_text=\"You are a helpful and knowledgeable agent. Only Answer if the {documents} contain the answer. If the user question is not related to the provided {documents}, say I don't have an answer\\n\"\n",
    "            \"Question: {query}\\n\"\n",
    "            \"Answer:\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:33:46.931729Z",
     "end_time": "2023-04-16T22:33:47.749148Z"
    }
   },
   "outputs": [],
   "source": [
    "# declare the QA pipeline's prompt node\n",
    "QA_builder = PromptNode(model_name_or_path=\"text-davinci-003\", api_key=openai_api_key, default_prompt_template=QA_promptnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:33:49.609080Z",
     "end_time": "2023-04-16T22:33:49.613002Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the QA pipeline tool\n",
    "from haystack import Pipeline\n",
    "\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=QA_builder, name=\"Generator\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:41:03.759133Z",
     "end_time": "2023-04-16T22:41:04.213792Z"
    }
   },
   "outputs": [],
   "source": [
    "from haystack_memory.prompt_templates import memory_template\n",
    "from haystack.agents import Agent, Tool\n",
    "from haystack.nodes import PromptNode\n",
    "\n",
    "# create agent prompt node and define our Agent \"memory_agent\"\n",
    "prompt_node = PromptNode(model_name_or_path=\"text-davinci-003\", api_key=openai_api_key, max_length=512,\n",
    "                         stop_words=[\"Observation:\"])\n",
    "memory_agent = Agent(prompt_node=prompt_node, prompt_template=memory_template)\n",
    "# Define the first tool: A document store QA tool based on the pipeline\n",
    "search_tool = Tool(name=\"DocumentStore_QA\",\n",
    "                   pipeline_or_node=pipe,\n",
    "                   description=\"Access this tool to find missing information needed to answer questions.\",\n",
    "                   output_variable=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:41:04.564809Z",
     "end_time": "2023-04-16T22:41:04.571905Z"
    }
   },
   "outputs": [],
   "source": [
    "from haystack_memory.memory import MemoryRecallNode\n",
    "\n",
    "memory_database = []\n",
    "memory_node = MemoryRecallNode(memory=memory_database)\n",
    "memory_tool = Tool(name=\"Memory\",\n",
    "                   pipeline_or_node=memory_node,\n",
    "                   description=\"Your memory. Always access this tool first to remember what you have learned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:41:06.499060Z",
     "end_time": "2023-04-16T22:41:06.503336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the memory and the search tool to the agent\n",
    "memory_agent.add_tool(search_tool)\n",
    "memory_agent.add_tool(memory_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:41:08.669081Z",
     "end_time": "2023-04-16T22:41:19.037155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent memory-shot-react started with {'query': \"where did zeus' golden sandals rest?\", 'params': None}\n",
      "\u001B[32m understand\u001B[0m\u001B[32m what\u001B[0m\u001B[32m the\u001B[0m\u001B[32m question\u001B[0m\u001B[32m is\u001B[0m\u001B[32m asking\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Memory\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m Input\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Where\u001B[0m\u001B[32m did\u001B[0m\u001B[32m Zeus\u001B[0m\u001B[32m'\u001B[0m\u001B[32m golden\u001B[0m\u001B[32m sand\u001B[0m\u001B[32mals\u001B[0m\u001B[32m rest\u001B[0m\u001B[32m?\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\u001B[0mObservation: \u001B[33m[]\u001B[0m\n",
      "Thought: \u001B[32m This\u001B[0m\u001B[32m is\u001B[0m\u001B[32m not\u001B[0m\u001B[32m enough\u001B[0m\u001B[32m.\u001B[0m\u001B[32m Let\u001B[0m\u001B[32m's\u001B[0m\u001B[32m try\u001B[0m\u001B[32m another\u001B[0m\u001B[32m tool\u001B[0m\u001B[32m.\u001B[0m\u001B[32m \u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Document\u001B[0m\u001B[32mStore\u001B[0m\u001B[32m_\u001B[0m\u001B[32mQ\u001B[0m\u001B[32mA\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m Input\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Where\u001B[0m\u001B[32m did\u001B[0m\u001B[32m Zeus\u001B[0m\u001B[32m'\u001B[0m\u001B[32m golden\u001B[0m\u001B[32m sand\u001B[0m\u001B[32mals\u001B[0m\u001B[32m rest\u001B[0m\u001B[32m?\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: \u001B[33mZeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief.\u001B[0m\n",
      "Thought: \u001B[32m This\u001B[0m\u001B[32m is\u001B[0m\u001B[32m the\u001B[0m\u001B[32m answer\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mFinal\u001B[0m\u001B[32m Answer\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Zeus\u001B[0m\u001B[32m'\u001B[0m\u001B[32m golden\u001B[0m\u001B[32m sand\u001B[0m\u001B[32mals\u001B[0m\u001B[32m rested\u001B[0m\u001B[32m upon\u001B[0m\u001B[32m a\u001B[0m\u001B[32m foot\u001B[0m\u001B[32mst\u001B[0m\u001B[32mool\u001B[0m\u001B[32m decorated\u001B[0m\u001B[32m with\u001B[0m\u001B[32m an\u001B[0m\u001B[32m Amazon\u001B[0m\u001B[32momach\u001B[0m\u001B[32my\u001B[0m\u001B[32m in\u001B[0m\u001B[32m relief\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\u001B[0m"
     ]
    }
   ],
   "source": [
    "from haystack_memory.utils import MemoryUtils\n",
    "\n",
    "# Chat with the agent\n",
    "memory_utils = MemoryUtils(memory_database=memory_database, agent=memory_agent)\n",
    "result, conversation_memory = memory_utils.chat(\"where did zeus' golden sandals rest?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:41:28.794066Z",
     "end_time": "2023-04-16T22:41:28.797265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"where did zeus' golden sandals rest?\", \"Zeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief.\"]\n"
     ]
    }
   ],
   "source": [
    "print(conversation_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:41:29.681238Z",
     "end_time": "2023-04-16T22:41:29.687621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"where did zeus' golden sandals rest?\", 'answers': [<Answer {'answer': \"Zeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief.\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>], 'transcript': \"You are a helpful agent. To achieve your goal of answering questions correctly, use step by step thinking and\\nuse the following tools to look for answers: DocumentStore_QA: Access this tool to find missing information needed to answer questions.\\nMemory: Your memory. Always access this tool first to remember what you have learned.. Give the tool the full question as input; the tools will respond with observations.\\nRefrain from using prior internal knowledge or tools you don't have access to. You should avoid searching elsewhere and avoid using external sources.\\nOnly use observations returned by the tools you have been given access to.\\nUse the following format:\\n\\nQuestion: the question to be answered\\nThought: Break down the problem into smaller steps. Only use the tools you have been given access to.\\n Tool: use one of the following tools to look for answers: DocumentStore_QA: Access this tool to find missing information needed to answer questions.\\nMemory: Your memory. Always access this tool first to remember what you have learned.. Always access the memory tool first.\\nTool Input: the full updated question to be answered\\nObservation: the tool will respond with an observation. If the observation is empty pick another tool to get more information.\\nFinal Answer: Your answer to the question. If the observations provided do not contain the answer or the tools you have access to respond with I don't have an answer, then respond with 'I don't have enough information to answer the question'\\nQuestion: where did zeus' golden sandals rest?\\nThought: Let's work this out it a step by step. I first need to  understand what the question is asking.\\nTool: Memory\\nTool Input: Where did Zeus' golden sandals rest?\\n\\nObservation: []\\nThought: This is not enough. Let's try another tool. \\nTool: DocumentStore_QA\\nTool Input: Where did Zeus' golden sandals rest?\\n\\n\\nObservation: Zeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief.\\nThought: This is the answer.\\nFinal Answer: Zeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief.\"}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-16T22:41:34.179612Z",
     "end_time": "2023-04-16T22:41:44.005150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent memory-shot-react started with {'query': 'which muslim general raided the city of Rhodes?', 'params': None}\n",
      "\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Memory\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m Input\u001B[0m\u001B[32m:\u001B[0m\u001B[32m which\u001B[0m\u001B[32m mus\u001B[0m\u001B[32mlim\u001B[0m\u001B[32m general\u001B[0m\u001B[32m raided\u001B[0m\u001B[32m the\u001B[0m\u001B[32m city\u001B[0m\u001B[32m of\u001B[0m\u001B[32m Rhodes\u001B[0m\u001B[32m?\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\u001B[0mObservation: \u001B[33m[\"where did zeus' golden sandals rest?\", \"Zeus' golden sandals rested upon a footstool decorated with an Amazonomachy in relief.\"]\u001B[0m\n",
      "Thought: \u001B[32m This\u001B[0m\u001B[32m is\u001B[0m\u001B[32m not\u001B[0m\u001B[32m relevant\u001B[0m\u001B[32m to\u001B[0m\u001B[32m the\u001B[0m\u001B[32m question\u001B[0m\u001B[32m I\u001B[0m\u001B[32m am\u001B[0m\u001B[32m trying\u001B[0m\u001B[32m to\u001B[0m\u001B[32m answer\u001B[0m\u001B[32m.\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Document\u001B[0m\u001B[32mStore\u001B[0m\u001B[32m_\u001B[0m\u001B[32mQ\u001B[0m\u001B[32mA\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mTool\u001B[0m\u001B[32m Input\u001B[0m\u001B[32m:\u001B[0m\u001B[32m which\u001B[0m\u001B[32m mus\u001B[0m\u001B[32mlim\u001B[0m\u001B[32m general\u001B[0m\u001B[32m raided\u001B[0m\u001B[32m the\u001B[0m\u001B[32m city\u001B[0m\u001B[32m of\u001B[0m\u001B[32m Rhodes\u001B[0m\u001B[32m?\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32m\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: \u001B[33mMuawiyah I\u001B[0m\n",
      "Thought: \u001B[32m This\u001B[0m\u001B[32m is\u001B[0m\u001B[32m the\u001B[0m\u001B[32m general\u001B[0m\u001B[32m that\u001B[0m\u001B[32m I\u001B[0m\u001B[32m am\u001B[0m\u001B[32m looking\u001B[0m\u001B[32m for\u001B[0m\u001B[32m!\u001B[0m\u001B[32m\n",
      "\u001B[0m\u001B[32mFinal\u001B[0m\u001B[32m Answer\u001B[0m\u001B[32m:\u001B[0m\u001B[32m Mu\u001B[0m\u001B[32maw\u001B[0m\u001B[32miyah\u001B[0m\u001B[32m I\u001B[0m\u001B[32m\u001B[0m"
     ]
    }
   ],
   "source": [
    "result, conversation_memory = memory_utils.chat(\"which muslim general raided the city of Rhodes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
